# ğŸ‰ WEB CRAWLER - START HERE!

## Welcome! Everything is Ready to Use ğŸš€

**Congratulations!** You have a complete, production-ready web crawler with:
- âœ… **Beautiful Web Interface** (React + Tailwind CSS)
- âœ… **Powerful Backend API** (Python + Flask)  
- âœ… **Command Line Tool** (CLI with interactive mode)
- âœ… **Docker Deployment** (One command to rule them all)
- âœ… **Full Documentation** (12 guides and references)

## ğŸ¯ Choose Your Adventure

### ğŸŒ I Want the Web Interface (Recommended for Beginners)

**Using Docker** (Easiest - Everything in one command):
```bash
docker-compose up -d
```
Then open http://localhost:3000 in your browser! ğŸŠ

**Without Docker** (Need Node.js and Python):
```bash
# Terminal 1: Start backend
cd backend
pip install -r requirements.txt
python -m flask --app api.app run

# Terminal 2: Start frontend
cd frontend
npm install
npm run dev

# Open http://localhost:3000
```

### ğŸ’» I Want to Use the Command Line (CLI)

```bash
# Navigate to backend
cd backend

# Install dependencies (first time only)
pip install -r requirements.txt

# Extract content from a webpage
python main.py --url https://example.com

# Extract links as JSON
python main.py --url https://example.com --mode link --format json

# Process multiple URLs from CSV
python main.py --csv urls.csv
```

### ğŸ”§ I Want to Use the API Directly

```bash
# Start the API
cd backend
python -m flask --app api.app run

# In another terminal, make requests
curl -X POST http://localhost:5000/api/crawl/single \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "mode": "content", "formats": ["txt", "md"]}'
```

### ğŸ³ I Have Docker and Want Everything Now

```bash
# One command to start everything:
docker-compose up -d

# Access these:
# - Frontend UI: http://localhost:3000
# - Backend API: http://localhost:5000/api
# - API Health: http://localhost:5000/health
```

## âš¡ 30-Second Quick Start

**Fastest way to see it work:**

```bash
# If you have Docker:
docker-compose up -d
# Open http://localhost:3000 â†’ Click "Start Crawling" â†’ Enter a URL â†’ Click "Start Crawling"

# If you don't have Docker:
cd backend
pip install -r requirements.txt
python main.py --url https://example.com
# Check the output/ folder for results!
```

## ğŸ“š What Can It Do?

### Content Extraction Mode
- Extract clean text from any webpage
- Save as **TXT**, **Markdown**, or **HTML**
- Download images alongside content
- Target specific sections (by CSS class or ID)
- Get detailed extraction metadata

### Link Extraction Mode
- Extract all links from a webpage
- Filter by type (all, internal only, external only)
- Save as **TXT** list or **JSON** with metadata
- Get link statistics

### Bulk Processing
- Upload CSV file with multiple URLs
- Process hundreds of URLs at once
- Get aggregate reports
- Individual output folders per URL

## ğŸ® Try These Examples

### Example 1: Simple Content Extraction
```bash
cd backend
python main.py --url https://example.com
# Check output/ folder for results
```

### Example 2: Markdown with Images
```bash
python main.py --url https://example.com/blog --format md --download-images
```

### Example 3: Extract All Links as JSON
```bash
python main.py --url https://example.com --mode link --format json
```

### Example 4: Bulk Process from CSV
```bash
python main.py --csv example_urls.csv
```

### Example 5: Use the Web Interface
```bash
docker-compose up -d
# Open http://localhost:3000
# Click around, it's intuitive!
```

## ğŸ“– Need More Help?

### Step-by-Step Guides
- **GETTING_STARTED.md** - Comprehensive setup guide (all platforms)
- **FRONTEND_QUICKSTART.md** - Frontend-specific quick start
- **QUICK_REFERENCE.md** - Command cheat sheet

### Understanding the Code
- **ARCHITECTURE.md** - System design and architecture
- **dev-spec.md** - Original specification
- **backend/README.md** - Backend documentation
- **frontend/README.md** - Frontend documentation

### When Things Go Wrong
- **TROUBLESHOOTING.md** - Common issues and solutions
- **DOCKER_FIX.md** - Docker-specific troubleshooting

### Project Status
- **PROJECT_COMPLETE.md** - Full completion summary
- **PHASE6_COMPLETE.md** - Frontend implementation details
- **STATUS.md** - Implementation tracking

## ğŸ› ï¸ Installation (If Not Yet Installed)

### Automated Installation

**Linux/Mac:**
```bash
chmod +x install.sh
./install.sh
```

**Windows:**
```bash
install.bat
```

### Manual Installation

**Backend:**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

**Frontend:**
```bash
cd frontend
npm install
```

**Environment:**
```bash
cp .env.example .env
cp frontend/.env.example frontend/.env
```

## ğŸ¯ Common Use Cases

### Use Case 1: Archive Blog Posts
```bash
python main.py --url https://blog.example.com/post --format md --download-images
```

### Use Case 2: Collect Links for SEO Analysis
```bash
python main.py --url https://example.com --mode link --format json
```

### Use Case 3: Extract Main Content Only
```bash
python main.py --url https://news.com/article --class article-content --format txt
```

### Use Case 4: Bulk Archive Multiple Pages
```bash
# Create urls.csv with your URLs
python main.py --csv urls.csv --output ./archive/
```

### Use Case 5: API Integration
```bash
curl -X POST http://localhost:5000/api/crawl/single \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "mode": "content",
    "formats": ["txt", "md", "html"],
    "download_images": true
  }'
```

## ğŸ” Project Structure Overview

```
web-crawler/
â”œâ”€â”€ backend/          â† Python backend with CLI
â”‚   â”œâ”€â”€ crawler/      â† Core crawling logic
â”‚   â”œâ”€â”€ api/          â† Flask REST API
â”‚   â”œâ”€â”€ utils/        â† Utilities
â”‚   â”œâ”€â”€ tests/        â† Unit tests
â”‚   â””â”€â”€ main.py       â† CLI entry point
â”‚
â”œâ”€â”€ frontend/         â† React web interface
â”‚   â”œâ”€â”€ src/          â† React components
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ services/
â”‚   â””â”€â”€ public/       â† Static assets
â”‚
â”œâ”€â”€ output/           â† Crawl results (auto-created)
â”œâ”€â”€ docker-compose.yml â† Docker orchestration
â”œâ”€â”€ .env              â† Environment variables
â””â”€â”€ [12 docs]         â† Comprehensive guides
```

## ğŸŠ Features Highlights

### Web Interface Features
- ğŸ¨ Modern, responsive design (works on mobile!)
- ğŸš€ Real-time progress tracking
- ğŸ“Š Visual statistics and charts
- ğŸ“ Drag-and-drop CSV upload
- â¬‡ï¸ Direct file downloads
- ğŸ“œ Extraction history
- ğŸ¯ Mode selection (Content/Link)
- âš™ï¸ All options in an intuitive form

### CLI Features
- ğŸ–¥ï¸ Interactive mode with prompts
- ğŸ¨ Colored output for better UX
- ğŸ“Š Progress indicators
- ğŸ“ Comprehensive help text
- ğŸš€ Fast and lightweight
- ğŸ”§ Scriptable for automation

### API Features
- ğŸŒ RESTful design
- ğŸ“¡ 9 endpoints
- ğŸ“Š Real-time status polling
- ğŸ“ File downloads
- ğŸ“œ Job history
- ğŸ—‘ï¸ Job management
- ğŸ”’ CORS support

## ğŸ› Quick Troubleshooting

### "Port already in use"
```bash
# Change ports in .env file
BACKEND_PORT=5001
FRONTEND_PORT=3001
```

### "Module not found"
```bash
cd backend
pip install -r requirements.txt
```

### "npm install fails"
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
```

### Docker issues
```bash
docker-compose down
docker-compose up -d --build
```

## ğŸ”— Quick Links

| What | Where |
|------|-------|
| **Frontend UI** | http://localhost:3000 |
| **Backend API** | http://localhost:5000/api |
| **API Health Check** | http://localhost:5000/health |
| **Output Files** | `./output/` directory |
| **Example CSV** | `example_urls.csv` |
| **Main Docs** | `README.md` |
| **Setup Guide** | `GETTING_STARTED.md` |
| **Troubleshooting** | `TROUBLESHOOTING.md` |

## âœ¨ Pro Tips

1. **Use Docker** for the easiest setup
2. **Interactive CLI** is great for learning: `python main.py`
3. **Web interface** is best for visual feedback
4. **API** is perfect for integrations
5. **Check output/** folder to see your crawled content
6. **Read extraction_details.json** for full metadata
7. **Use --help** to see all CLI options: `python main.py --help`

## ğŸ‰ You're Ready!

Pick one of the options above and start crawling! 

The easiest way is:
```bash
docker-compose up -d
```
Then go to http://localhost:3000 and start clicking! ğŸ–±ï¸

Need help? Check the documentation files listed above. Everything is explained in detail.

**Happy Crawling!** ğŸ•·ï¸ğŸ•¸ï¸

---

**Project Status:** âœ… 100% Complete | **Version:** 1.0.0 | **Last Updated:** Dec 24, 2025
